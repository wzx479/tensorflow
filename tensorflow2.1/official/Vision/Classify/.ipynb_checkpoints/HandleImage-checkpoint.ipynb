{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pathlib\n",
    "import math\n",
    "\n",
    "from models.resnet import resnet_18, resnet_34, resnet_50, resnet_101, resnet_152"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some training parameters\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 8\n",
    "NUM_CLASSES = 3\n",
    "image_height = 224\n",
    "image_width = 224\n",
    "channels = 3\n",
    "save_model_dir = \"saved_model/model\"\n",
    "dataset_dir = \"dataset/\"\n",
    "train_dir = dataset_dir + \"train\"\n",
    "valid_dir = dataset_dir + \"valid\"\n",
    "test_dir = dataset_dir + \"test\"\n",
    "\n",
    "# choose a network\n",
    "model = \"resnet18\"\n",
    "# model = \"resnet34\"\n",
    "# model = \"resnet50\"\n",
    "# model = \"resnet101\"\n",
    "# model = \"resnet152\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(img_path):\n",
    "    # read pictures\n",
    "    img_raw = tf.io.read_file(img_path)\n",
    "    # decode pictures\n",
    "    img_tensor = tf.image.decode_jpeg(img_raw, channels=channels)\n",
    "    # resize\n",
    "    img_tensor = tf.image.resize(img_tensor, [image_height, image_width])\n",
    "    img_tensor = tf.cast(img_tensor, tf.float32)\n",
    "    # normalization\n",
    "    img = img_tensor / 255.0\n",
    "    return img\n",
    "\n",
    "def get_images_and_labels(data_root_dir):\n",
    "    # get all images' paths (format: string)\n",
    "    data_root = pathlib.Path(data_root_dir)\n",
    "    all_image_path = [str(path) for path in list(data_root.glob('*/*'))]\n",
    "    # get labels' names\n",
    "    label_names = sorted(item.name for item in data_root.glob('*/'))\n",
    "    # dict: {label : index}\n",
    "    label_to_index = dict((label, index) for index, label in enumerate(label_names))\n",
    "    # get all images' labels\n",
    "    all_image_label = [label_to_index[pathlib.Path(single_image_path).parent.name] for single_image_path in all_image_path]\n",
    "\n",
    "    return all_image_path, all_image_label\n",
    "\n",
    "\n",
    "def get_dataset(dataset_root_dir):\n",
    "    all_image_path, all_image_label = get_images_and_labels(data_root_dir=dataset_root_dir)\n",
    "    # print(\"image_path: {}\".format(all_image_path[:]))\n",
    "    # print(\"image_label: {}\".format(all_image_label[:]))\n",
    "    # load the dataset and preprocess images\n",
    "    image_dataset = tf.data.Dataset.from_tensor_slices(all_image_path).map(load_and_preprocess_image)\n",
    "    label_dataset = tf.data.Dataset.from_tensor_slices(all_image_label)\n",
    "    dataset = tf.data.Dataset.zip((image_dataset, label_dataset))\n",
    "    image_count = len(all_image_path)\n",
    "\n",
    "    return dataset, image_count\n",
    "\n",
    "\n",
    "def generate_datasets():\n",
    "    train_dataset, train_count = get_dataset(dataset_root_dir=config.train_dir)\n",
    "    valid_dataset, valid_count = get_dataset(dataset_root_dir=config.valid_dir)\n",
    "    test_dataset, test_count = get_dataset(dataset_root_dir=config.test_dir)\n",
    "\n",
    "\n",
    "    # read the original_dataset in the form of batch\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=train_count).batch(batch_size=config.BATCH_SIZE)\n",
    "    valid_dataset = valid_dataset.batch(batch_size=config.BATCH_SIZE)\n",
    "    test_dataset = test_dataset.batch(batch_size=config.BATCH_SIZE)\n",
    "\n",
    "    return train_dataset, valid_dataset, test_dataset, train_count, valid_count, test_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = resnet_50()\n",
    "    if config.model == \"resnet18\":\n",
    "        model = resnet_18()\n",
    "    if config.model == \"resnet34\":\n",
    "        model = resnet_34()\n",
    "    if config.model == \"resnet101\":\n",
    "        model = resnet_101()\n",
    "    if config.model == \"resnet152\":\n",
    "        model = resnet_152()\n",
    "    model.build(input_shape=(None, config.image_height, config.image_width, config.channels))\n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the original_dataset\n",
    "train_dataset, valid_dataset, test_dataset, train_count, valid_count, test_count = generate_datasets()\n",
    "\n",
    "# create model\n",
    "model = get_model()\n",
    "\n",
    "# define loss and optimizer\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adadelta()\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "valid_loss = tf.keras.metrics.Mean(name='valid_loss')\n",
    "valid_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='valid_accuracy')\n",
    "\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images, training=True)\n",
    "        loss = loss_object(y_true=labels, y_pred=predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "\n",
    "@tf.function\n",
    "def valid_step(images, labels):\n",
    "    predictions = model(images, training=False)\n",
    "    v_loss = loss_object(labels, predictions)\n",
    "\n",
    "    valid_loss(v_loss)\n",
    "    valid_accuracy(labels, predictions)\n",
    "\n",
    "# start training\n",
    "for epoch in range(config.EPOCHS):\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    valid_loss.reset_states()\n",
    "    valid_accuracy.reset_states()\n",
    "    step = 0\n",
    "    for images, labels in train_dataset:\n",
    "        step += 1\n",
    "        train_step(images, labels)\n",
    "        print(\"Epoch: {}/{}, step: {}/{}, loss: {:.5f}, accuracy: {:.5f}\".format(epoch + 1,\n",
    "                                                                                 config.EPOCHS,\n",
    "                                                                                 step,\n",
    "                                                                                 math.ceil(train_count / config.BATCH_SIZE),\n",
    "                                                                                 train_loss.result(),\n",
    "                                                                                 train_accuracy.result()))\n",
    "\n",
    "    for valid_images, valid_labels in valid_dataset:\n",
    "        valid_step(valid_images, valid_labels)\n",
    "\n",
    "    print(\"Epoch: {}/{}, train loss: {:.5f}, train accuracy: {:.5f}, \"\n",
    "          \"valid loss: {:.5f}, valid accuracy: {:.5f}\".format(epoch + 1,\n",
    "                                                              config.EPOCHS,\n",
    "                                                              train_loss.result(),\n",
    "                                                              train_accuracy.result(),\n",
    "                                                              valid_loss.result(),\n",
    "                                                              valid_accuracy.result()))\n",
    "\n",
    "model.save_weights(filepath=config.save_model_dir, save_format='tf')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
